{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Scene Recognition with Bag-of-Words](https://www.cc.gatech.edu/~hays/compvision/proj4/)\n",
    "For this project, you will need to report performance for three\n",
    "combinations of features / classifiers. It is suggested you code them in\n",
    "this order, as well:\n",
    "1. Tiny image features and nearest neighbor classifier\n",
    "2. Bag of sift features and nearest neighbor classifier\n",
    "3. Bag of sift features and linear SVM classifier\n",
    "\n",
    "The starter code is initialized to 'placeholder' just so that the starter\n",
    "code does not crash when run unmodified and you can get a preview of how\n",
    "results are presented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up parameters, image paths and category list\n",
    "%matplotlib notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "import pickle\n",
    "from random import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "import student_code as sc\n",
    "\n",
    "\n",
    "# This is the list of categories / directories to use. The categories are\n",
    "# somewhat sorted by similarity so that the confusion matrix looks more\n",
    "# structured (indoor and then urban and then rural).\n",
    "categories = ['Kitchen', 'Store', 'Bedroom', 'LivingRoom', 'Office', 'Industrial', 'Suburb',\n",
    "              'InsideCity', 'TallBuilding', 'Street', 'Highway', 'OpenCountry', 'Coast',\n",
    "              'Mountain', 'Forest'];\n",
    "# This list of shortened category names is used later for visualization\n",
    "abbr_categories = ['Kit', 'Sto', 'Bed', 'Liv', 'Off', 'Ind', 'Sub',\n",
    "                   'Cty', 'Bld', 'St', 'HW', 'OC', 'Cst',\n",
    "                   'Mnt', 'For'];\n",
    "\n",
    "# Number of training examples per category to use. Max is 100. For\n",
    "# simplicity, we assume this is the number of test cases per category, as\n",
    "# well.\n",
    "num_train_per_cat = 100\n",
    "\n",
    "# This function returns lists containing the file path for each train\n",
    "# and test image, as well as lists with the label of each train and\n",
    "# test image. By default all four of these lists will have 1500 elements\n",
    "# where each element is a string.\n",
    "data_path = osp.join('..', 'data')\n",
    "train_image_paths, test_image_paths, train_labels, test_labels = get_image_paths(data_path,\n",
    "                                                                                 categories,\n",
    "                                                                                 num_train_per_cat);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Tiny Image features with Nearest Neighbor classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1a: Represent each image with the Tiny Image feature\n",
    "\n",
    "Each function to construct features should return an N x d numpy array, where N is the number of paths passed to the function and d is the dimensionality of each image representation. See the starter code for each function for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Using the TINY IMAGE representation for images')\n",
    "\n",
    "train_image_feats = sc.get_tiny_images(train_image_paths)\n",
    "test_image_feats = sc.get_tiny_images(test_image_paths)\n",
    "\n",
    "print(\"Shape of training features array:\", train_image_feats.shape)\n",
    "print(\"Shape of testing features array:\", test_image_feats.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1b: Classify each test image by training and using the Nearest Neighbor classifier\n",
    "\n",
    "Each function to classify test features will return an N element list, where N is the number of test cases and each entry is a string indicating the predicted category for each test image. Each entry in 'predicted_categories' must be one of the 15 strings in 'categories', 'train_labels', and 'test_labels'. See the starter code for each function for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Using NEAREST NEIGHBOR classifier to predict test set categories')\n",
    "\n",
    "predicted_categories = sc.nearest_neighbor_classify(train_image_feats, train_labels, test_image_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1c: Build a confusion matrix and score the recognition system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(You do not need to code anything in this section.)\n",
    "\n",
    "If we wanted to evaluate our recognition method properly we would train\n",
    "and test on many random splits of the data. You are not required to do so\n",
    "for this project.\n",
    "\n",
    "This function will create a confusion matrix and various image\n",
    "thumbnails each time it is called. View the confusion matrix to help interpret\n",
    "your classifier performance. Where is it making mistakes? Are the\n",
    "confusions reasonable?\n",
    "\n",
    "Interpreting your performance with 100 training examples per category:\n",
    "- accuracy  =   0 -> Your code is broken (probably not the classifier's fault! A classifier would have to be amazing to perform this badly).\n",
    "- accuracy ~= .07 -> Your performance is chance. Something is broken or you ran the starter code unchanged.\n",
    "- accuracy ~= .20 -> Rough performance with tiny images and nearest neighbor classifier. Performance goes up a few percentage points with K-NN instead of 1-NN.\n",
    "- accuracy ~= .20 -> Rough performance with tiny images and linear SVM classifier. The linear classifiers will have a lot of trouble trying to separate the classes and may be unstable (e.g. everything classified to one category)\n",
    "- accuracy ~= .50 -> Rough performance with bag of SIFT and nearest neighbor classifier. Can reach .60 with K-NN and different distance metrics.\n",
    "- accuracy ~= .60 -> You've gotten things roughly correct with bag of SIFT and a linear SVM classifier.\n",
    "- accuracy >= .70 -> You've also tuned your parameters well. E.g. number of clusters, SVM regularization, number of patches sampled when building vocabulary, size and step for dense SIFT features.\n",
    "- accuracy >= .80 -> You've added in spatial information somehow or you've added additional, complementary image features. This represents state of the art in Lazebnik et al 2006.\n",
    "- accuracy >= .85 -> You've done extremely well. This is the state of the art in the 2010 SUN database paper from fusing many  features. Don't trust this number unless you actually measure many random splits.\n",
    "- accuracy >= .90 -> You used modern deep features trained on much larger image databases.\n",
    "- accuracy >= .96 -> You can beat a human at this task. This isn't a realistic number. Some accuracy calculation is broken or your classifier is cheating and seeing the test labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "show_results(train_image_paths, test_image_paths, train_labels, test_labels, categories, abbr_categories,\n",
    "             predicted_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Bag of SIFT features with Nearest Neighbor classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2a: Represent each image with the Bag of SIFT feature\n",
    "\n",
    "To create a new vocabulary, make sure `vocab_filename` is different than the old vocabulary, or delete the old one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the BAG-OF-SIFT representation for images\n"
     ]
    }
   ],
   "source": [
    "print('Using the BAG-OF-SIFT representation for images')\n",
    "\n",
    "vocab_size = 400\n",
    "vocab_filename = 'vocab.pkl'\n",
    "if not osp.isfile(vocab_filename):\n",
    "    # Construct the vocabulary\n",
    "    print('No existing visual word vocabulary found. Computing one from training images')\n",
    "    vocab = sc.build_vocabulary(train_image_paths, vocab_size)\n",
    "    with open(vocab_filename, 'wb') as f:\n",
    "        pickle.dump(vocab, f)\n",
    "        print('{:s} saved'.format(vocab_filename))\n",
    "\n",
    "train_image_feats = sc.get_bags_of_sifts(train_image_paths, vocab_filename)\n",
    "test_image_feats = sc.get_bags_of_sifts(test_image_paths, vocab_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2b: Classify each test image by training and using the Nearest Neighbor classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Using NEAREST NEIGHBOR classifier to predict test set categories')\n",
    "predicted_categories = sc.nearest_neighbor_classify(train_image_feats, train_labels, test_image_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2c: Build a confusion matrix and score the recognition system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "show_results(train_image_paths, test_image_paths, train_labels, test_labels, categories, abbr_categories,\n",
    "             predicted_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation section\n",
    "In this section, cross-validation is used to tune the vocabulary size hyper-parameter. My approach is to split the training data into 60/40 splits and use the 40% split as a validation set. The following code will perform 10 iterations for each choice of vocabulary size (with different 60/40 splits randomly sampled each time) and compute the average accuracy and standard deviation on the validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "num_train_per_cat_cross_val = 25\n",
    "data_path = osp.join('..', 'data')\n",
    "train_image_paths_cross_val, _, train_labels_cross_val, _ = get_image_paths(data_path, \\\n",
    "                                                                  categories, num_train_per_cat_cross_val);\n",
    "\n",
    "iters_per_param_value    =  10\n",
    "candidate_vocab_sizes    =  [10, 20, 50, 100, 150, 200, 400, 600, 1000, 5000, 10000]\n",
    "candidate_lambda_values  =  [0.1, 1, 10, 100, 200, 500, 750, 900, 1000, 1200, 1500, 2000]\n",
    "num_training_images      =  len(train_image_paths_cross_val)\n",
    "train_percentage         =  0.6 # validation_percentage = 1.0 - train_percentage\n",
    "num_training_split       =  int(train_percentage*num_training_images)\n",
    "\n",
    "np_paths_array           =  np.array(train_image_paths_cross_val) # for being able to randomly access indices\n",
    "np_labels_array          =  np.array(train_labels_cross_val)\n",
    "cat2idx                  =  {cat: idx for idx, cat in enumerate(categories)}\n",
    "\n",
    "mean_acc_list          = [] # for plotting error bars\n",
    "std_acc_list           = [] # for plotting error bars\n",
    "best_cand_lambdas_list = []\n",
    "\n",
    "for i in range(len(candidate_vocab_sizes)):\n",
    "    accuracy_list = [] # store accuracy values to compute mean and std acc over iterations \n",
    "    lambdas_vote = np.zeros((len(candidate_lambda_values),1))\n",
    "    print(\"trying candidate vocab size:\",candidate_vocab_sizes[i])\n",
    "    for j in range(iters_per_param_value):\n",
    "        \n",
    "        # Generate random indices for training images:\n",
    "        rnd_indxs = np.random.choice(num_training_images, num_training_images, replace=False)\n",
    "        \n",
    "        # Depending on train/validation percentage split, separate training and validation images:\n",
    "        train_image_paths_split       =  np_paths_array[rnd_indxs[:num_training_split]]\n",
    "        validation_image_paths_split  =  np_paths_array[rnd_indxs[num_training_split:]]\n",
    "        train_labels_split            =  np_labels_array[rnd_indxs[:num_training_split]]\n",
    "        validation_labels_split       =  np_labels_array[rnd_indxs[num_training_split:]]\n",
    "        \n",
    "        # Create vocabulary for current train/validation split:\n",
    "        vocab_filename = 'cross_validation_data/vocab_' + \\\n",
    "                         str(candidate_vocab_sizes[i]) + '_iter_' + str(j+1) + '.pkl'\n",
    "        if not osp.isfile(vocab_filename):\n",
    "            vocab = sc.build_vocabulary(train_image_paths_split, candidate_vocab_sizes[i])\n",
    "            with open(vocab_filename, 'wb') as f:\n",
    "                pickle.dump(vocab, f)\n",
    "\n",
    "        # Use the generated vocabulary to extract bags-of-sift features from train & validation images:\n",
    "        train_features        =  sc.get_bags_of_sifts(train_image_paths_split, vocab_filename)\n",
    "        validation_features   =  sc.get_bags_of_sifts(validation_image_paths_split, vocab_filename)  \n",
    "        \n",
    "        y_true                =  [cat2idx[cat] for cat in validation_labels_split]\n",
    "        \n",
    "        # for current vocabulary size test possible candidate lambdas:\n",
    "        testing_lambdas_list  =  []\n",
    "        for k in range(len(candidate_lambda_values)):\n",
    "            predicted_categories  =  sc.svm_classify(train_features, train_labels_split, \\\n",
    "                                                     validation_features, lambda_value=candidate_lambda_values[k])\n",
    "        \n",
    "            # Create a confusion matrix, compute accuracy and store in a list:\n",
    "            y_pred  =  [cat2idx[cat] for cat in predicted_categories]\n",
    "            cm      =  confusion_matrix(y_true, y_pred)\n",
    "            cm      =  cm.astype(np.float) / cm.sum(axis=1)[:, np.newaxis]\n",
    "            acc     =  np.mean(np.diag(cm))\n",
    "            testing_lambdas_list.append(acc)\n",
    "        best_lambda_indx = np.argmax(np.array(testing_lambdas_list))\n",
    "        lambdas_vote[best_lambda_indx,0] += 1\n",
    "        \n",
    "        accuracy_list.append(testing_lambdas_list[best_lambda_indx]) # Store accuracy corresponding to best lambda\n",
    "        print(\"Iteration:\", j+1, \"best acc:\", accuracy_list[-1],\\\n",
    "              \"best lambda:\", candidate_lambda_values[best_lambda_indx])\n",
    "        \n",
    "    acc_list_array = np.array(accuracy_list)    \n",
    "    mean_acc_list.append(np.mean(acc_list_array))\n",
    "    std_acc_list.append(np.std(acc_list_array))\n",
    "        \n",
    "    highest_voted_lambda_indx = np.argmax(lambdas_vote[:,0])\n",
    "    best_cand_lambdas_list.append(candidate_lambda_values[highest_voted_lambda_indx])\n",
    "    \n",
    "    print(\"Stats: mean acc = \", mean_acc_list[-1], \"std acc = \", std_acc_list[-1], \\\n",
    "          \"highest voted lambda = \", best_cand_lambdas_list[-1])\n",
    "\n",
    "cross_val_means   = np.array(mean_acc_list)\n",
    "cross_val_stds    = np.array(std_acc_list)\n",
    "cross_val_lambdas = np.array(best_cand_lambdas_list) \n",
    "np.savez('cross_validation_results', means=cross_val_means, stds=cross_val_stds, lambdas=cross_val_lambdas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.plot_cross_validation_results_vocab_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coarse-to-fine for picking lambda \n",
    "Based on the results from cross-validation above, the tuned vocabulary size is used to fine tune the lambda parameter which controls how strongly regularized the model is. The range is determined based on the highest voted lambda in the cross-validation section above. \n",
    "#### NOTE: RUN CELL IN SECTION 2b BEFORE RUNNING THE CELL BELOW "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "candidate_lambda_values       =  np.arange(1,1000,10)\n",
    "train_labels_array            =  np.array(train_labels)\n",
    "iters                         =  10\n",
    "\n",
    "# split training data into train and validation:\n",
    "train_split                   =  0.7\n",
    "total_images                  =  train_image_feats.shape[0]\n",
    "num_train_images_split        =  int(train_split*total_images)\n",
    "cat2idx                       =  {cat: idx for idx, cat in enumerate(categories)}\n",
    "\n",
    "mean_acc_list          = [] # for plotting error bars\n",
    "std_acc_list           = [] # for plotting error bars\n",
    "\n",
    "for i in range(candidate_lambda_values.shape[0]):\n",
    "    temp_acc_list=[]\n",
    "    for j in range(iters):\n",
    "        rnd_indxs                     =  np.random.choice(total_images, total_images, replace=False)\n",
    "        train_image_feats_split       =  train_image_feats[rnd_indxs[:num_train_images_split], :]\n",
    "        validation_image_feats_split  =  train_image_feats[rnd_indxs[num_train_images_split:], :]\n",
    "        train_labels_split            =  train_labels_array[rnd_indxs[:num_train_images_split]]\n",
    "        validation_labels_split       =  train_labels_array[rnd_indxs[num_train_images_split:]]\n",
    "    \n",
    "        y_true                        =  [cat2idx[cat] for cat in validation_labels_split]\n",
    "        predicted_categories          =  sc.svm_classify(train_image_feats_split, train_labels_split, \\\n",
    "                                           validation_image_feats_split, \\\n",
    "                                           lambda_value=candidate_lambda_values[i])\n",
    "        \n",
    "        # build confusion matrix and compute prediction accuracy on validation data:\n",
    "        y_pred  =  [cat2idx[cat] for cat in predicted_categories]\n",
    "        cm      =  confusion_matrix(y_true, y_pred)\n",
    "        cm      =  cm.astype(np.float) / cm.sum(axis=1)[:, np.newaxis]\n",
    "        acc     =  np.mean(np.diag(cm))\n",
    "        temp_acc_list.append(acc)\n",
    "        sys.stdout.write(\"trial:%d/%d, iter:%d, acc:%f\\r\" % (i+1, candidate_lambda_values.shape[0], \\\n",
    "                                                             j+1, temp_acc_list[-1]))\n",
    "        sys.stdout.flush()\n",
    "    mean_acc_list.append(np.mean(np.array(temp_acc_list)))\n",
    "    std_acc_list.append(np.std(np.array(temp_acc_list)))\n",
    "    \n",
    "cross_val_lambda_means = np.array(mean_acc_list)\n",
    "cross_val_lambda_stds  = np.array(std_acc_list)\n",
    "np.savez('cross_validation_lambda_results', means=cross_val_lambda_means, stds=cross_val_lambda_stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_results_data = np.load('cross_validation_lambda_results.npz')\n",
    "\n",
    "X    = np.arange(1,1000,10)\n",
    "Y    = cross_val_results_data['means']\n",
    "Yerr = cross_val_results_data['stds']\n",
    "\n",
    "print(\"best acc:\",Y[np.argmax(Y)],\"for lambda:\", X[np.argmax(Y)])\n",
    "\n",
    "plt.figure()\n",
    "plt.errorbar(X,Y,Yerr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Bag of SIFT features and SVM classifier\n",
    "We will reuse the bag of SIFT features from Section 2a.\n",
    "\n",
    "The difference is that this time we will classify them with a support vector machine (SVM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3a: Classify each test image by training and using the SVM classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Using SVM classifier to predict test set categories')\n",
    "predicted_categories = sc.svm_classify(train_image_feats, train_labels, test_image_feats, lambda_value=645)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3b: Build a confusion matrix and score the recognition system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_results(train_image_paths, test_image_paths, train_labels, test_labels, categories, abbr_categories,\n",
    "             predicted_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
