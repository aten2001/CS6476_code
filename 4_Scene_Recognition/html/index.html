<html>
<head>
<title>Computer Vision Project</title>
<link href='http://fonts.googleapis.com/css?family=Nunito:300|Crimson+Text|Droid+Sans+Mono' rel='stylesheet' type='text/css'>
<link rel="stylesheet" title="Default" href="styles/github.css">
<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js"></script>

<link rel="stylesheet" href="highlighting/styles/default.css">
<script src="highlighting/highlight.pack.js"></script>

<script type="text/javascript" src="http://latex.codecogs.com/latexit.js"></script>
<script type="text/javascript">
LatexIT.add('li',true);
</script>
<script type="text/javascript">
LatexIT.add('p',true);
</script>

<style type="text/css">
body {
	margin: 0px;
	width: 100%;
	font-family: 'Crimson Text', serif;
	font-size: 20px;
	background: #fcfcfc;
}
h1 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 28px;
	margin: 25px 0px 0px 0px;
	text-transform: lowercase;

}

h2 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 32px;
	margin: 15px 0px 35px 0px;
	color: #333;
	word-spacing: 3px;
}

h3 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 26px;
	margin: 10px 0px 10px 0px;
	color: #333;
	word-spacing: 2px;
}
h4 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 22px;
	margin: 10px 0px 10px 0px;
	color: #333;
	word-spacing: 2px;
}

h5 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 18px;
	margin: 10px 0px 10px 0px;
	color: #111;
	word-spacing: 2px;
}

p, li {
	color: #444;
}

a {
	color: #DE3737;
}

.container {
	margin: 0px auto 0px auto;
	width: 1160px;
}

#header {
	background: #333;
	width: 100%;
}

#headersub {
	color: #ccc;
	width: 960px;
	margin: 0px auto 0px auto;
	padding: 20px 0px 20px 0px;
}

.chart {
	width: 480px;
}
.lol {
	font-size: 16px;
	color: #888;
	font-style: italic;
}
.sep {
	height: 1px;
	width: 100%;
	background: #999;
	margin: 20px 0px 20px 0px;
}
.footer{
	font-size: 16px;
}
/*.latex {
	width: 100%;
}

.latex img {
	display: block;
	margin: 0px auto 0px auto;
}*/

pre {
	font-family: 'Droid Sans Mono';
	font-size: 14px;
}

table td {
  text-align: center;
  vertical-align: middle;
}

table td img {
  text-align: center;
  vertical-align: middle;
}

#contents a {
}
</style>
<script type="text/javascript">
    hljs.initHighlightingOnLoad();
</script>
</head>
<body>
<div id="header" >
<div id="headersub">
<h1>Marcus Pereira</h1>
</div>
</div>
<div class="container">

<h2> Project 4: Scene recognition with bag of words</h2>

<h2>Brief overview of algorithm implementation</h2>

<p> In this project, I have implemented a scene recognition pipeline that can extract 2 kinds of features namely, tiny images and bag-of-sift features which can then be used by 2 kinds of classifiers, namely Nearest Neighbors (1-NN or K-NN) and linear SVM classifiers. Following is a brief summary of my methodology for each of these: </p>

<ol>
<li><span lang="latex">\textbf{\underline{Section 1.a \(Tiny Image feature\)}:-}</span>To construct tiny image features, my code first loads each image using the provided image paths and resizes the loaded images using OpenCV's resize function: <code>cv2.resize(img,(16,16),interpolation=cv2.INTER_AREA)</code>. The 2D matrix of image pixel intensities in then flattened into a vector and a transform is applied depending on whether I want to "standardize" or "normalize" the feature. In standardization, the mean intensity of the pixels is computed and subtracted from each pixel intensity, thus centering the intensities about zero. Then the standard deviation is computed and each zero-centered pixel intensity is scaled by the standard deviation. In normalization, the same zero centering is performed but the intensities are scaled by the norm of the un-transformed feature vector so that the resulting vector has unit length. I found through my experiments that standardizing the feature vector increased prediction accuracy when used with my implementation of a K-NN classifier. </li>	
<li><span lang="latex">\textbf{\underline{Section 1.b \(Nearest Neighbor Classifier\)}:-}</span>My code allows using either 1-NN or K-NN classification. In 1-NN classification, pairwise distances between each test feature and all training image features are computed using the following sklearn function: <code>sklearn_pairwise.pairwise_distances(test_image_feats,train_image_feats)</code> and each test feature is assigned the same label as the "nearest" training label (shortest euclidean distance using numpy's <code>np.argmin()</code>). In k-NN classification, for each test feature the above computed pairwise distances are first sorted and the first "k" (hyper-parameter) distances (or corresponding training labels) are selected. Then using the following Numpy function: <code>np.unique(nearest_labels,return_counts=True)</code> unique labels (amongst the k selected labels) and their corresponding counts are obtained. This serves as a simple voting scheme to pick the training label with the most votes (or counts) that will be assigned to the particular test feature being considered. This process is repeated for all the test features. Through trail-and-error, I found that setting k=18 resulted in high accuracy for k-NN classification with tiny image features.</li>
<li><span lang="latex">\textbf{\underline{Section 2.b \(Bag-of-SIFT feature\)}:-}</span>In order to generate a bag-of-sift feature representation a vocabulary of visual words has to be generated. In my implementation of <code>build_vocabulary()</code>, the images are loaded using the provided image paths and SIFT features are extracted using <code> vlfeat.sift.dsift(img,step=10,fast=True)</code>from each image. These are then used to build a visual word vocabulary using k-means clustering that outputs <code>vocab_size</code> (hyper-paramter) number of cluster centers representing the visual vocabulary. The hyper-parameter is selected by a cross-validation process that will be detailed later in this report. The vocabulary is stored in a pickle file and can be used to construct bag-of-SIFT feature vectors of training, validation and testing images, implemented in <code>get_bags_of_sifts()</code>. This process involves the following steps: dense (step size of 5) SIFT feature extraction from images, assigning the extracted features to cluster centers of the vocabulary using <code>vlfeat.kmeans.kmeans_quantize()</code> and finally using these assignments to construct a normalized histogram with the number of bins equal to <code>vocab_size</code>.</li>
<li><span lang="latex">\textbf{\underline{Section 3.b \(Linear SVM classifier\)}:-}</span>In this section, my code uses sklearn's LinearSVC() class to train 15 1-vs-all linear classifiers for each of the 15 scene categories. The hyper-parameter "lambda" (penalty paramter) is determined by a cross-validation approach detailed later in this report. After training the 15 classifiers, each test image feature is tested with all the 15 classifiers. This is done by concatinating the <code>coef_</code> and <code>intercept_</code> attributes of each SVM class into a matrix W and vector b respectively. Then a simple matrix-vector multiplication (W^T * feature + b) yields the confidence scores for all 15 categories for the particular test label. The label of the category with the highest confidence score is assigned to the test feature.</li>
</ol>

<h2>Cross-Validation for determining the hyper-parameters</h2>
<p>There are 2 important hyper-parameters in this project that must be tuned for high accuracy on the test data, namely, vocabulary size and lambda (SVM penalty parameter). Since the process of building a vocabulary is very slow, I use 1/4th of the total training images used for the final evaluation (i.e. only 25 images of each category). Next, a list of candidate vocabulary sizes is created to iterate through and evaluate the performance on the validation set. However, the process of prediction and evaluation on the validation dataset depends on the SVM penalty parameter (lambda). Therefore, my strategy was to pick a list of coarse candidate lambda values. For each candidate vocabulary size, my implementation, performs 10 iterations of building a vocabulary, constructing bag-of-SIFT features using the vocabulary and then iterating through each of the coarse candidate lambda values to predict the labels in the validation set. The lambda value that provides the highest accuracy is voted and the accuracy stored to compute statistics for the particular value of vocabulary size over 10 iterations. In each of the 10 iterations (for each candidate vocabulary size) training data indices are first randomly sampled (without replacement) and the random samples are split into 60% training and 40% validation datasets. At the end of 10 iterations (for each candidate vocabulary size), the stored highest accuracies are used to compute the accuracy mean and standard deviation for the particular candidate vocabulary size as well as the lambda with highest votes is stored. After iterating through the entire list of candidate vocabulary sizes, an errorbar plot is generated with the stored accuracy means and standard deviations and the candidate with the highest mean accuracy is selected. The highest voted lambda value corresponding to the highest mean accuracy is also returned. </p> 

<p>Ideally, the highest voted lambda value must be used to perform a finer search in its vicinity for a lambda that yields an even higher accuracy. However, my implementation performs the following coarse-to-fine cross-validation for tuning lambda. Using the tuned vocabulary size, a new vocabulary is built and bag-of-SIFT features of the entire training set are constructed. A finer list of candidate lambda values is generated to iterate over. Similar to the process above, the code iterates through this list of candidate values, training SVMs by randomly sampling indices and creating training and validation sets (70/30 splits) for 20 iterations (for each candidate) and stores the computed accuracies. After 20 iterations for each candidate the accuracy mean and standard deviation are computed and stored. After iterating through all the candidate values, the stored statistics are used to generate an errorbar plot and the lambda with the highest mean accuray is reported. This can be used to train SVMs on all of the 1500 image training data and test on the 1500 testing images. I found that by further testing for lambdas in the vicinity of the reported lambda, the accuracy and be improved by a small amount.</p>

<p>Justification of strategy:</p>
<ol>
<li>First, I try to justify using a coarse grid of lambdas to tune the vocabulary size parameter. Since, initially both the hyper-parameters are unknown, and the SVM predictions are very sensitive to the choice of lambdas, having a coarse grid allows some chance of getting a high/moderate accuracy prediction for a particular vocabulary size being tested. Further more, if the combination of the particular vocabulary size and lambda is indeed an optimal choice, then over iterations of randomly sampling (using the same vocabulary size) the same lambda is likely to result in highest accuracy often and as a result end up receiving the highest number of votes. Now, the correct procedure here would be to perform not only 10, but many more iterations (maybe 100), compute the votes of each lambda, select the highest voted lambda and use those corresponding accuracies to compute the statistics for the vocabulary size in question.</li>
</ol>

<h2>Algorithm performance and discussion on results (only report accuracy, except the best performing recognition)</h2>

<ol>
<li>Tiny image features (un-transformed) with 1-NN classifier vs k-NN classifier</li>
<p>1-NN classifier prediction accuracy: $20.93\%$, k-NN classifier prediction accuracy: $19.33\%$ with k = 18</p>

<li>Tiny image features (standardized) with 1-NN classifier vs k-NN classifier</li>	
<p>1-NN classifier prediction accuracy: $22.13\%$, k-NN classifier prediction accuracy: $24.93\%$ with k = 11 (hand tuned best value)</p>

<li>Bag-of-SIFT features with 1-NN classifier vs k-NN classifier</li>	
<li>Bag-of-SIFT features with LSVM classifier (include confusion matrix)</li>	
</ol>

<!-- <div style="clear:both">
<h3>Example heading</h3>

<p> 	Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.</p>

<h2>Example of code with highlighting</h2>
The javascript in the <code>highlighting</code> folder is configured to do syntax highlighting.<p>

<h3>Results in a table</h3>

<table border=1>
<tr>
<td>
<img src="placeholder.jpg" width="24%"/>
<img src="placeholder.jpg"  width="24%"/>
<img src="placeholder.jpg" width="24%"/>
<img src="placeholder.jpg" width="24%"/>
</td>
</tr>

<tr>
<td>
<img src="placeholder.jpg" width="24%"/>
<img src="placeholder.jpg"  width="24%"/>
<img src="placeholder.jpg" width="24%"/>
<img src="placeholder.jpg" width="24%"/>
</td>
</tr>

</table>

<div style="clear:both" >
<p> 	Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.</p>
</div>
 -->
</body>
</html>
