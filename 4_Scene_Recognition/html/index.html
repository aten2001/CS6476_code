<html>
<head>
<title>Computer Vision Project</title>
<link href='http://fonts.googleapis.com/css?family=Nunito:300|Crimson+Text|Droid+Sans+Mono' rel='stylesheet' type='text/css'>
<link rel="stylesheet" title="Default" href="styles/github.css">
<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js"></script>

<link rel="stylesheet" href="highlighting/styles/default.css">
<script src="highlighting/highlight.pack.js"></script>

<script type="text/javascript" src="http://latex.codecogs.com/latexit.js"></script>
<script type="text/javascript">
LatexIT.add('li',true);
</script>
<script type="text/javascript">
LatexIT.add('p',true);
</script>

<style type="text/css">
body {
	margin: 0px;
	width: 100%;
	font-family: 'Crimson Text', serif;
	font-size: 20px;
	background: #fcfcfc;
}
h1 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 28px;
	margin: 25px 0px 0px 0px;
	text-transform: lowercase;

}

h2 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 32px;
	margin: 15px 0px 35px 0px;
	color: #333;
	word-spacing: 3px;
}

h3 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 26px;
	margin: 10px 0px 10px 0px;
	color: #333;
	word-spacing: 2px;
}
h4 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 22px;
	margin: 10px 0px 10px 0px;
	color: #333;
	word-spacing: 2px;
}

h5 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 18px;
	margin: 10px 0px 10px 0px;
	color: #111;
	word-spacing: 2px;
}

p, li {
	color: #444;
}

a {
	color: #DE3737;
}

.container {
	margin: 0px auto 0px auto;
	width: 1160px;
}

#header {
	background: #333;
	width: 100%;
}

#headersub {
	color: #ccc;
	width: 960px;
	margin: 0px auto 0px auto;
	padding: 20px 0px 20px 0px;
}

.chart {
	width: 480px;
}
.lol {
	font-size: 16px;
	color: #888;
	font-style: italic;
}
.sep {
	height: 1px;
	width: 100%;
	background: #999;
	margin: 20px 0px 20px 0px;
}
.footer{
	font-size: 16px;
}
/*.latex {
	width: 100%;
}

.latex img {
	display: block;
	margin: 0px auto 0px auto;
}*/

pre {
	font-family: 'Droid Sans Mono';
	font-size: 14px;
}

table td {
  text-align: center;
  vertical-align: middle;
}

table td img {
  text-align: center;
  vertical-align: middle;
}

#contents a {
}
</style>
<script type="text/javascript">
    hljs.initHighlightingOnLoad();
</script>
</head>
<body>
<div id="header" >
<div id="headersub">
<h1>Marcus Pereira</h1>
</div>
</div>
<div class="container">

<h2> Project 4: Scene recognition with bag of words</h2>

<h2>Brief overview of algorithm implementation</h2>

<p> In this project, I have implemented a scene recognition pipeline that can extract 2 kinds of features namely, tiny images and bag-of-sift features which can then be used by 2 kinds of classifiers, namely Nearest Neighbors (1-NN or K-NN) and linear SVM classifiers. Following is a brief summary of my methodology for each of these: </p>

<ol>

<li><span lang="latex">\textbf{\underline{Section 1.a \(Tiny Image feature\)}:-}</span>To construct tiny image features, my code first loads each image using the provided image paths and resizes the loaded images using OpenCV's resize function: <code>cv2.resize(img,(16,16),interpolation=cv2.INTER_AREA)</code>. The 2D matrix of image pixel intensities in then flattened into a vector and a transform is applied depending on whether I want to "standardize" or "normalize" the feature. In standardization, the mean intensity of the pixels is computed and subtracted from each pixel intensity, thus centering the intensities about zero. Then the standard deviation is computed and each zero-centered pixel intensity is scaled by the standard deviation. In normalization, the same zero centering is performed but the intensities are scaled by the norm of the zero-centered feature vector so that the resulting vector has unit length. </li>	

<li><span lang="latex">\textbf{\underline{Section 1.b \(Nearest Neighbor Classifier\)}:-}</span>My code allows using either 1-NN or K-NN classification. 
	
	<p> In 1-NN classification, pairwise distances between each test feature and all training image features are computed using the following sklearn function: <code>sklearn_pairwise.pairwise_distances(
	test_image_feats,train_image_feats)</code> and each test feature is assigned the same label as the "nearest" training label (shortest euclidean distance using numpy's <code>np.argmin()</code>). </p>
	
	<p>In k-NN classification, for each test feature the above computed pairwise distances are first sorted and the first "k" (hand tuned hyper-parameter) distances (or corresponding training labels) are selected. Then using the following Numpy function: <code>np.unique(nearest_labels,return_counts=True)</code> unique labels (amongst the k selected labels) and their corresponding counts and indices are obtained. This serves as a simple voting scheme to pick the training label with the most votes (or counts) that will be assigned to the particular test feature being considered. This process is repeated for all the test features. The unique label corresponding to the highest count (or most votes) is assigned to the test feature. However, in case of ties (i.e. multiple unique labels having same "highest count"), the unique indices of these counts are used to obtain corresponding indices in the originally computed pairwise distance vector. The ties are then resolved by selecting the label that corresponds to the lowest computed pairwise distance and assigning that to the test feature.</p>
</li>

<li><span lang="latex">\textbf{\underline{Section 2.b \(Bag-of-SIFT feature\)}:-}</span>In order to generate a bag-of-SIFT feature representation a vocabulary of visual words has to be generated. In my implementation of <code>build_vocabulary()</code>, the images are loaded using the provided image paths and SIFT features are extracted using <code> vlfeat.sift.dsift(img,step=10,fast=True)</code>from each image. These are then used to build a visual word vocabulary using k-means clustering that outputs <code>vocab_size</code> (hyper-paramter) number of cluster centers representing the visual vocabulary. The hyper-parameter is selected by a cross-validation process that will be detailed later in this report. The vocabulary is stored in a pickle file and can be used to construct bag-of-SIFT feature vectors of training, validation and testing images, implemented in <code>get_bags_of_sifts()</code>. This process involves the following steps: dense (step size of 5) SIFT feature extraction from images, assigning the extracted features to cluster centers of the vocabulary using <code>vlfeat.kmeans.kmeans_quantize()</code> and finally using these assignments to construct a normalized histogram with the number of bins equal to <code>vocab_size</code>.</li>
<li><span lang="latex">\textbf{\underline{Section 3.b \(Linear SVM classifier\)}:-}</span>In this section, my code uses sklearn's LinearSVC() class to train 15 1-vs-all linear classifiers for each of the 15 scene categories. The hyper-parameter "lambda" (penalty paramter) is determined by a cross-validation approach detailed later in this report. After training the 15 classifiers, each test image feature is tested with all the 15 classifiers. This is done by concatinating the <code>coef_</code> and <code>intercept_</code> attributes of each SVM class into a matrix $W$ and vector $b$ respectively. Then a simple matrix-vector multiplication $(W^T * feature + b)$ yields the confidence scores for all 15 categories for the particular test label. The label of the category with the highest confidence score is assigned to the test feature.</li>
</ol>

<h2>Cross-Validation for determining the hyper-parameters</h2>
<p>There are 2 important hyper-parameters in this project that must be tuned for high accuracy on the test data, namely, vocabulary size and lambda (SVM penalty parameter). Since the process of building a vocabulary is very slow, I use 1/4th of the total training images used for the final evaluation (i.e. only 25 images of each category). Next, a list of candidate vocabulary sizes is created to iterate through and evaluate the performance on the validation set. However, the process of prediction and evaluation on the validation set depends on the SVM penalty parameter (lambda). Therefore, my strategy was to pick a list of coarse candidate lambda values. For each candidate vocabulary size, my implementation, performs 10 iterations of building a vocabulary, constructing bag-of-SIFT features using the vocabulary and then iterating through each of the coarse candidate lambda values to predict the labels in the validation set. The lambda value that provides the highest accuracy receives a vote and the accuracy value is stored to compute statistics after completion of 10 iterations. In each of the 10 iterations (for each candidate vocabulary size) training data indices are first randomly sampled (without replacement) and the random samples are split into 60% training and 40% validation datasets. At the end of 10 iterations (for each candidate vocabulary size), the stored highest accuracies are used to compute the accuracy mean and standard deviation for the particular candidate vocabulary size as well as the lambda with highest votes is stored. After iterating through the entire list of candidate vocabulary sizes, an errorbar plot is generated with the stored accuracy means and standard deviations and the candidate with the highest mean accuracy is selected. The highest voted lambda value corresponding to the highest mean accuracy is also returned. </p> 

<p>Highest accuracy: $\mathbf{0.540212343212}$ for vocabulary size of $\mathbf{400}$</p>
<figure>
<img src="cross_val_vocab.png" width=60%>
<figcaption>Errorbar plot showing cross-validation results for tuning vocabulary size hyperparamter</figcaption>
</figure>


<p>Ideally, the highest voted lambda value must be used to perform a finer search in its vicinity for a lambda that yields an even higher accuracy. However, instead of a coarse-to-fine search, I used an additional cross-validation process for tuning lambda. I have tried to justify this below. Using the tuned vocabulary size, a new vocabulary is built and bag-of-SIFT features of the entire training set are constructed. A finer list of candidate lambda values is generated to iterate over. Similar to the process above, the code iterates through this list of candidate values, training SVMs by randomly sampling indices and creating training and validation sets (70/30 splits) for 20 iterations (for each candidate) and stores the computed accuracies. After 20 iterations for each candidate the accuracy mean and standard deviation are computed and stored. After iterating through all the candidate values, the stored statistics are used to generate an errorbar plot and the lambda with the highest mean accuray is reported. </p>

<p>Highest mean acc: $\mathbf{0.679809569738}$ for lambda: $\mathbf{591}$</p>

<figure>
<img src="cross_val_lambda.png" width=60%>
<figcaption>Errorbar plot showing cross-validation results for tuning lambda (SVM penalty) hyperparamter</figcaption>
</figure>

<p>Justification of strategy:</p>
<ol>
<li>First, I try to justify using a coarse grid of lambdas to tune the vocabulary size parameter. Since, initially both the hyper-parameters are unknown, and the SVM predictions are very sensitive to the choice of lambdas, having a coarse grid of possible lambdas allows some chance of achieving a high/moderate accuracy prediction for a particular vocabulary size being tested. Further more, if the combination of the particular vocabulary size and lambda is indeed an optimal choice, then over iterations of randomly sampling training image indices (using the same vocabulary size) the same lambda is likely to result in highest accuracy more often and as a result end up receiving the highest number of votes. Now, the correct procedure here would be to perform not only 10, but many more iterations (maybe 100), compute the votes of each lambda, select the highest voted lambda and use those corresponding accuracies to compute the statistics for the vocabulary size in question. However, this being a very time instensive process I performed only 10 iterations and computed statistics using all the best lambdas. This is my justification of why I don't use the corresponding highest voted lambda after the cross-validation step for tuning vocabulary size, but perform another cross-validation for lambda using the tuned vocabulary size parameter.</li>
<li>Secondly, I try to explain "what I would've done" for a research project. I would use some form of parallelization to simulaneously evaluate candidates of both vocabulary size and lambdas on finer grids. Essentially, randomly sample image paths every iteration, build vocabulary based on candidate value, extract bag-of-SIFT features of train and validation data, test each candidate lambda in a nested loop, evaluate accuracy on validation set and vote for best lambda, compute statistics for candidate vocabulary size based on accuracies of highest voted lambdas and finally return the candidate vocabulary size with best statistics and corresponding highest voted lambda.</li>
</ol>

<h2>Reporting algorithm performance</h2>
<p><span lang="latex">\textbf{Please note that for the results below, I commented out shuffle(pth) in utils.py in order to get deterministic (first 100 images) evaluations on every run.}</span>
<span lang="latex">\textbf{The values of k below have been hand-tuned by trying different values and evaluating on test images (no cross-validation for k-NN).}</span></p>
<ol>
<li>Tiny image features (un-transformed) with 1-NN classifier vs k-NN classifier</li>
<p>1-NN classifier prediction accuracy: $\mathbf{20.53\%}$, k-NN classifier prediction accuracy: $\mathbf{21.73\%}$ with $\mathbf{k = 4}$</p>

<li>Tiny image features (standardized) with 1-NN classifier vs k-NN classifier</li>	
<p>1-NN classifier prediction accuracy: $\mathbf{23.40\%}$, k-NN classifier prediction accuracy: $\mathbf{24.53\%}$ with $\mathbf{k = 12}$ </p>

<li>Tiny image features (unit-norm) with 1-NN classifier vs k-NN classifier</li>	
<p>1-NN classifier prediction accuracy: $\mathbf{23.40\%}$, k-NN classifier prediction accuracy: $\mathbf{24.53\%}$ with $\mathbf{k = 12}$ (same results as with standardization) </p>

<li>Bag-of-SIFT features with 1-NN classifier vs k-NN classifier</li>	
<p>1-NN classifier prediction accuracy: $\mathbf{53.80\%}$, k-NN classifier prediction accuracy: $\mathbf{55.93\%}$ with $\mathbf{k = 4}$ </p>

<li>Bag-of-SIFT features with LSVM classifier</li>
<p>From performing cross-validation for tuning <code>vocab_size</code>, the value obtained is <code>vocab_size = 400</code>. And performing a subsequent cross-validation using this value of vocabulary size for lambdas, the value obtained is <code>lambda = 591</code>.</p>	
<p>The resulting accuracy on the test set: $\mathbf{71.67\%}$. This is without shuffling the image paths so that I pick the same 100 images everytime. Following is the confusion matrix:</p>
		<figure>
		  <img src="final_confusion_mat.png" width=75%>
		</figure>

<li>Monte Carlo testing results (last cell in jupyter notebook): Bag-of-SIFT features with LSVM classifier with shuffle(pth)</li>
<p>Using tuned hyper-parameters, 20 trials of randomly sampling 1500 training and testing images each and evaluation result in the following statistics: Mean Prediction Accuracy = $\mathbf{0.6983}$  and Standard Deviation = $\mathbf{0.00742809980188}$ </p>
</ol>

<!-- <div style="clear:both">
<h3>Example heading</h3>

<p> 	Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.</p>

<h2>Example of code with highlighting</h2>
The javascript in the <code>highlighting</code> folder is configured to do syntax highlighting.<p>

<h3>Results in a table</h3>

<table border=1>
<tr>
<td>
<img src="placeholder.jpg" width="24%"/>
<img src="placeholder.jpg"  width="24%"/>
<img src="placeholder.jpg" width="24%"/>
<img src="placeholder.jpg" width="24%"/>
</td>
</tr>

<tr>
<td>
<img src="placeholder.jpg" width="24%"/>
<img src="placeholder.jpg"  width="24%"/>
<img src="placeholder.jpg" width="24%"/>
<img src="placeholder.jpg" width="24%"/>
</td>
</tr>

</table>

<div style="clear:both" >
<p> 	Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.</p>
</div>
 -->
</body>
</html>
